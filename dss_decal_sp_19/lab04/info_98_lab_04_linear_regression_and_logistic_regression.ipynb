{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 98: Data Science Skills, Spring 2019\n",
    "## Lab 04: Linear Regression and Logistic Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Setup](#setup)\n",
    "* [Linear Regression](#linreg)\n",
    "    * Defining a Model\n",
    "    * Loss Functions\n",
    "    * Minimizing The Loss\n",
    "* [Logistic Regression](#logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "# Setup\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install plotly\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "py.init_notebook_mode(connected=False)\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linreg'></a>\n",
    "# 1. Linear Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Defining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loadsummary",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Loading the Tips Dataset\n",
    "\n",
    "To begin with, we load the tips dataset from the `seaborn` library.  The tips data contains records of tips, total bill, and information about the person who paid the bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loaddata",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"tips\")\n",
    "\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will attempt to model the tip value (in dollars) as a function of the total bill.  As a consequence we define the following mathematical model:\n",
    "\n",
    "$$\\Large\n",
    "\\texttt{Tip} = \\theta^*  \\times \\texttt{TotalBill}\n",
    "$$\n",
    "\n",
    "This follows the similar intuition that tips are some **unknown** percentage of the total bill.  We will then try to estimate the slope of this relationship which corresponds to the percent tip.\n",
    "\n",
    "Here the parameter $\\theta^*$ represents the true percent tip that we would like to estimate.  \n",
    "\n",
    "**Implement the python function for this model (yes this is very easy):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(theta, total_bill):\n",
    "    \"\"\"\n",
    "    Takes the parameter theta and the total bill, and returns the computed tip.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: tip percentage \n",
    "    total_bill: total bill value in dollars\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loss Functions\n",
    "\n",
    "In this lab we will implement the squared loss and the absolute loss functions.  \n",
    "Suppose for a given total bill $x$, we observe a tip value of $y$ and our model predicts a tip value $\\hat{y}$ by:\n",
    "$$\\Large\n",
    "% the \\hspace{0pt} is added to address a bug in safari mathjax\n",
    "\\hat{\\hspace{0pt}y} = \\theta x\n",
    "$$ \n",
    "then any of the following might be appropriate **loss functions**\n",
    "\n",
    "1. **Squared Loss** (also known as the $L^2$ loss pronounced \"ell-two\"):\n",
    "$$\\Large\n",
    "% the \\hspace{0pt} is added to address a bug in safari mathjax\n",
    "L\\left(y, \\hat{\\hspace{0pt}y} \\right) = \\left( y - \\hat{\\hspace{0pt}y} \\right)^2\n",
    "$$\n",
    "1. **Absolute Loss** (also known as the $L^1$ loss pronounced \"ell-one\"):\n",
    "$$\\Large\n",
    "% the \\hspace{0pt} is added to address a bug in safari mathjax\n",
    "L\\left(y, \\hat{\\hspace{0pt}y} \\right) = \\left| y - \\hat{\\hspace{0pt}y} \\right|\n",
    "$$\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "In this question, you are going to define functions for **squared loss** and **absolute loss**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.2a: Implement the squared loss function\n",
    "\n",
    "\n",
    "$$\\Large\n",
    "L\\left(y, \\hat{\\hspace{0pt}y} \\right) = \\left( y - \\hat{\\hspace{0pt}y} \\right)^2\n",
    "$$\n",
    "\n",
    "Using the comments below, implement the squared loss function. Your answer should not use any loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.2b: Plotting Squared Loss\n",
    "\n",
    "Suppose you observe a bill of \\\\$28 with a tip \\\\$3. (Does this tip look reasonable?)\n",
    "\n",
    "Transform this information in our model, we have a $y=3.00$ and $x=28.00$. Now suppose we pick an initial range of $\\theta$'s (tip percent in this case) for you. Use the `model` and `squared_loss` function defined above to plot the loss for a range of $\\theta$ values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "y = 3.00\n",
    "x = 28.00\n",
    "thetas = np.linspace(0, 0.3, 200) # A range of theta values\n",
    "\n",
    "## Finish this by replacing 0.0 with the correct calculation \n",
    "## Hint: You will use squared_loss y, model, theta and x\n",
    "#loss should be a numpy array where the ith entry corresponds to the loss for the ith theta\n",
    "loss = np.array([ 0.0 for theta in thetas])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-plot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To test your loss calculation above, run the cell below, and it should produce this picture:\n",
    "\n",
    "![squared loss](squared_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-makeplot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "student",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas, loss, label=\"Squared Loss\")\n",
    "plt.title(\"Squared Loss of Observed and Predicted Tip (in dollars)\")\n",
    "plt.xlabel(r\"Choice for $\\theta$ (tip percent)\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(\"squared_loss_my_plot.png\",  bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.2c: Implement the absolute loss \n",
    "\n",
    "$$\\Large\n",
    "L\\left(y, \\hat{\\hspace{0pt}y} \\right) = \\left| y - \\hat{\\hspace{0pt}y} \\right|\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def abs_loss(y_obs, y_hat):\n",
    "    \"\"\"\n",
    "    Calculate the absolute loss of the observed data and predicted data.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    y_obs: an array of observed values\n",
    "    y_hat: an array of predicted values\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    An array of loss values corresponding to the absolute loss for each prediction\n",
    "    \"\"\"\n",
    "    return np.abs(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c-plot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Below is the plot of the absolute loss.  If you implemented things correctly it should look like:\n",
    "\n",
    "![absolute loss](absolute_loss.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c-makeplot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "student",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "y = 3.00\n",
    "x = 28.00\n",
    "thetas = np.linspace(0, 0.3, 200) \n",
    "\n",
    "# Code provided for you this time. (you're welcome)\n",
    "loss = np.array([abs_loss(y, model(theta,x)) for theta in thetas])\n",
    "\n",
    "plt.plot(thetas, loss, label=\"Absolute Loss\")\n",
    "plt.title(\"Absolute Loss of Observed and Predicted Tip (in dollars)\")\n",
    "plt.xlabel(r\"Choice for $\\theta$ (tip percent)\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(\"absolute_loss_my_plot.png\",  bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## 1.2d: Plotting **Average Loss** for our Data\n",
    "Remember we define our model to be:\n",
    "$$\\Large\n",
    "% the \\hspace{0pt} is added to address a bug in safari mathjax\n",
    "\\hat{\\hspace{0pt}y} = \\theta x\n",
    "$$ \n",
    "Now, we can extend the above loss functions to an entire dataset by taking the average. Let the dataset $\\mathcal{D}$ be the set of observations:\n",
    "\n",
    "$$\\Large\n",
    "\\mathcal{D} = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}\n",
    "$$\n",
    "\n",
    "where $x_i$ is the total bill and $y_i$ is the tip dollar amount.\n",
    "\n",
    "We can define the average loss over the dataset as:\n",
    "\n",
    "$$\\Large\n",
    "L\\left(\\theta, \\mathcal{D}\\right) = \\frac{1}{n} \\sum_{i=1}^n L(m_\\theta(x_i), y_i) = \\frac{1}{n} \\sum_{i=1}^n L(\\theta x_i, y_i) = \\frac{1}{n} \\sum_{i=1}^n L(\\hat{y_i}, y_i)\n",
    "$$\n",
    "\n",
    "where $m_\\theta(x_i) = \\theta x_i = \\hat{y_i}$ is the model evaluated using the parameters $\\theta$ on the bill amount $x_i$.\n",
    "\n",
    "**Complete the following code block to render a plot of the average absolute and squared loss for different values of $\\theta$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "thetas = np.linspace(0, 0.3, 200) # A range of theta values\n",
    "y = data['tip']\n",
    "x = data['total_bill']\n",
    "\n",
    "# Replace 0.0 with the correct value computed \n",
    "# Use the model and loss functions from above\n",
    "\n",
    "# This time, each loss array should be a numpy array where the ith entry corresponds to the \n",
    "# average loss across all data points for the ith theta\n",
    "# consider using squared_los() and abs_loss functions\n",
    "avg_squared_loss = np.array([0.0 for theta in thetas])\n",
    "avg_absolute_loss = np.array([0.0 for theta in thetas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d-plot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To test your loss calculations, run the cell below. If your code was correct, the following plot should look like:\n",
    "\n",
    "![Average Loss](average_loss.png)\n",
    "\n",
    "Note: Your colors might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d-makeplot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "student",
     "written"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas, avg_squared_loss, label = \"Average Squared Loss\")\n",
    "plt.plot(thetas, avg_absolute_loss, label = \"Average Absolute Loss\")\n",
    "plt.title(\"Average Squared and Absolute Loss of Observed and Predicted Tip (in dollars)\")\n",
    "plt.xlabel(r\"Choice for $\\theta$ (tip percent)\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"average_loss_my_plot.png\",  bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-896580605adb2db2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Based on the plot above, approximately what is the optimal value of theta you would choose for this model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ccc8882852afdfbe",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "q2d = ... # answer question here as a string\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "q2d = \"\"\"We should choose ~0.15 because this is the value of theta that minimizes the loss functions.\"\"\"\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "<br/><br/><br/> \n",
    "\n",
    "## 1.3: Minimizing The Loss\n",
    "\n",
    "In some cases, it is possible to use calculus to analytically compute the parameters $\\theta$ that minimize the loss function.  However, in this lab we will use computational techniques to minimize the loss.  Here we will use the [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) routine to minimize the average loss.\n",
    "\n",
    "Complete the following python function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def minimize_average_loss(loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    Minimize the average loss calculated from using different thetas, and \n",
    "    find the estimation of theta for the model.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    loss_function: A loss function, can be the squared or absolute loss function from above.\n",
    "    model: A defined model function, here we use the model defined above\n",
    "    x: the x values (total bills)\n",
    "    y: the y values (tip amounts)\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimation for theta (tip percent) as a scalar\n",
    "    \n",
    "    Note we will ignore failed convergence for this lab ... \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Notes on the following function call which you need to finish:\n",
    "    # \n",
    "    # 0. the ... should be replaced with the average loss evaluated on \n",
    "    #       the data x, y using the model and appropriate loss function\n",
    "    # 1. x0 is the initial value for THETA.  Yes, this is confusing\n",
    "    #       but people who write optimization libraries like to use x  \n",
    "    #       as the variable name to optimize, not theta.\n",
    "    \n",
    "    return minimize(lambda theta: ..., x0=0.0)['x'][0] # We extract 'x' entry in dict, which contains optimal theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='losreg'></a>\n",
    "# 2. Logistic Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "dataset",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In this lab we will be working on the breast cancer dataset. This dataset can be easily loaded using the `sklearn.datasets.load_breast_cancer()` method.  \n",
    "The data format is not a `pandas.DataFrame` so we will create a new DataFrame from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loaddata",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = sklearn.datasets.load_breast_cancer()\n",
    "# data is actually a dictionnary\n",
    "print(data.keys())\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "dataframe",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "fitone",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let us try to fit a simple model with only one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "buildxy",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Define our features/target\n",
    "X = df[[\"mean radius\"]]\n",
    "# Target data['target'] = 0 is malignant 1 is benign\n",
    "Y = (data.target == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "split",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split between train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Training Data Size: {len(x_train)}\")\n",
    "print(f\"Test Data Size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "Now let us first fit a logistic regression model using the training set.  \n",
    "We will compute the training and testing accuracy, defined as:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\text{Training Accuracy} = \\frac{1}{n_{train\\_set}} \\sum_{i \\in \\text{train_set}} {\\mathbb{1}_{y_i == \\hat{y_i}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\text{Testing Accuracy} = \\frac{1}{n_{test\\_set}} \\sum_{i \\in \\text{test_set}} {\\mathbb{1}_{y_i == \\hat{y_i}}}\n",
    "$$\n",
    "\n",
    "where $ y_i $ is the prediction of our model, $ y_i $ the ground truth and $\\mathbb{1}_{y_i == \\hat{y_i}}$ an indicator function ($ \\mathbb{1}_{y_i == \\hat{y_i}} = 1 \\iff  y_i = \\hat{y_i}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sklearn.linear_model.LogisticRegression(fit_intercept=True)\n",
    "\n",
    "...\n",
    "train_accuracy = ...\n",
    "test_accuracy = ...\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "It seems we can a get very high test accuracy. Then how about precision and recall?  \n",
    "- Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.  \n",
    "- Recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.\n",
    "\n",
    "Precision is the ability of the classifier not to label as positive a sample that is negative while recall is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "To understand the link between recall/precision on the one hand and sensitivity/specificity on the other hand, it's useful to come back to a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, lr.predict(x_test))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "class_names = ['False', 'True']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "$$\n",
    "\\text{Precision} = \\frac{n_{true\\_positives}}{n_{true\\_positives} + n_{false\\_positives}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{n_{true\\_positives}}{n_{true\\_positives} + n_{false\\_negatives}}\n",
    "$$\n",
    "\n",
    "As illustrated in the figure below:\n",
    "![precision_recall](precision_recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the precision and recall for the test set using the model we got from question 1.  \n",
    "Please do not use the `sklearn.metrics` for this computation.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test) \n",
    "\n",
    "precision = ...\n",
    "recall = ...\n",
    "\n",
    "print(f'precision = {precision:.4f}')\n",
    "print(f'recall = {recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you infer from these results? Please consider the following plots, that display the distribution of the target variable in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "sns.countplot(y_train, ax=axes[0]);\n",
    "sns.countplot(y_test, ax=axes[1]);\n",
    "\n",
    "axes[0].set_title('Train')\n",
    "axes[1].set_title('Test')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3\n",
    "Now let's try to analyze the cross entropy loss. Recall that loss would be:\n",
    "$$\\Large L(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n \\left( y_i \\phi(x_i)^T \\theta + \\log \\left(\\sigma\\left(-\\phi(x_i)^T \\theta\\right) \\right) \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([lr.coef_[0][0],\n",
    "                  lr.intercept_[0]])\n",
    "Phi = np.hstack([X,\n",
    "                 np.ones([len(X), 1])])\n",
    "print(theta)\n",
    "print()\n",
    "print(Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_loss(theta, Phi, Y):\n",
    "    '''\n",
    "    Compute the cross entropy loss using Phi, Y and theta. Hint: # The notation B @ v means: \n",
    "    compute the matrix multiplication Bv \n",
    "\n",
    "    Args:\n",
    "        theta: The model parameters. \n",
    "        Phi: The transformed input data \\phi(X)\n",
    "        Y: The label \n",
    "\n",
    "    Return:\n",
    "        The cross entropy loss.\n",
    "    '''\n",
    "    loss = ...\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvalues = np.linspace(-8,8,70)\n",
    "vvalues = np.linspace(-5,5,70)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "lr_loss_values = np.array([lr_loss(t, Phi, Y) for t in thetas.T])\n",
    "lr_loss_surface = go.Surface(name=\"Logistic Regression Loss\",\n",
    "        x=u, y=v, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))),\n",
    "        contours=dict(z=dict(show=True, color=\"gray\", project=dict(z=True)))\n",
    "    )\n",
    "\n",
    "py.iplot(go.Figure(data=[lr_loss_surface]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What remarks can you make on this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
