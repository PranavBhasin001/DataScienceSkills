{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing, Statistical Analysis and Drawing Conclusions from data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducting stastical tests and testing hypotheses are some of the fundamentals when it comes to analyzing data and drawing conclusions. The NumPy library is a particuarly powerful tool for data manipulation and we will see it come into play in some of the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever analyzing a dataset or conducting a study to test the validity of a given hypothesis, the outcome may revise, reject or retain the hypothesis and this helps us determine the acceptability of the hypothesis and the theory from which it may be derived. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: The Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All statistical tests attempt to choose between two views of the world. Specifically, the choice is between two views about how the data were generated. These two views are called hypotheses.\n",
    "\n",
    "The null hypothesis. This is a clearly defined model about chances. It says that the data were generated at random under clearly specified assumptions about the randomness. The word \"null\" reinforces the idea that if the data look different from what the null hypothesis predicts, the difference is due to nothing but chance.\n",
    "\n",
    "From a practical perspective, the null hypothesis is a hypothesis under which you can simulate data.\n",
    "\n",
    "On the other hand, the alternative hypothesis states that some external factor (not chance) is responsible for the given data differing from the prediction made in the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the example of an experiment analyzing whether a given coin is fair or biased. On tossing this coin 2000 times, we get heads 950 times. We will aim to understand whether this is down to chance or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Null Hypothesis: The coin is fair, and any variation from the expected results (1000 heads) are due to only random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Alternative Hypothesis:  The coin is unfair and there is something other than chance causing the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a Python array to represent a fair coin, with the two possible outcomes being heads and tails. We can use Python to make choices at random and this is where NumPy comes in to play. In numpy, there is a sub-module called random that contains many functions that involve random selection. One of these functions is called choice. It picks one item at random from an array, and it is equally likely to pick any of the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = np.array(['Heads', 'Tails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_thousand_tosses = np.random.choice(coin, 2000) #equal chance of heads and tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(two_thousand_tosses == 'Heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: The Test Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide between the two hypothesis, we must choose a statistic that we can use to make the decision. This is called the test statistic. While a number of statistics may be appropriate in various situations, in our scenario exploring the fairness of the given coin we could simply use |the number of heads - 1000|. We would then compare the observed number of this test statistic to what we see under the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: The Distribution of the Test Statistic, Under the Null Hypothesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main computational aspect of testing hypotheses is figuring out what the values of the test statistic might be if the null hypothesis were true. The test statistic is simulated based on the assumptions of the model in the null hypothesis. That model involves chance, so the statistic comes out differently when you simulate it multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Python to create a simulation of tossing a fair coin 2000 times. In orded to increase the reliability of our simulation, it is important to repeat 2000 coin tosses as many times as possible. In order to do this 10000 times, we can use iteration which basically involves employing a for loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = np.array([]) #will collect our simulated values\n",
    "for i in np.arange(10000):\n",
    "    outcomes = np.random.choice(coin, 2000)\n",
    "    num_heads = np.count_nonzero(outcomes == 'Heads')\n",
    "    heads = np.append(heads, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: The Conclusion of the Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice between the null and alternative hypotheses depends on the comparison between what you computed in Steps 2 and 3: the observed value of the test statistic and its distribution as predicted by the null hypothesis. If the two are consistent with each other, then the observed test statistic is in line with what the null hypothesis predicts. If the data do not support the null hypothesis, we say that the test rejects the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding whether the observed value of the test statistic and its distribution under the null hypothesis are consistent (p-value, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_test_stat = abs(1000 - 950) #where 950 is the observed number of heads\n",
    "observed_test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37., 23., 18., ..., 14., 25., 22.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_statistic_simulation = abs(heads - 1000)\n",
    "test_statistic_simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll try to find the chance of getting heads less than, or equal to, 950 times or more than, or equal to, 1050 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0264"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(test_statistic_simulation >= observed_test_stat)/10000 #10000 is the number of reptitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chance that we would get heads 950 times or less, or 1050 times or more, is < 5% when we flip a fair coin 2000 times, under 10000 simulations. This chance is called the observed significance level of the test aka the P-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The P-value of a test is the chance, based on the model in the null hypothesis, that the test statistic will be equal to the observed value in the sample or even further in the direction that supports the alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a P-value is small, that means the tail beyond the observed statistic is small and so the observed statistic is far away from what the null predicts. This implies that the data supports the alternative hypothesis better than they support the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the P-value is less than 5%, it is considered small and the result is called \"statistically significant.\"\n",
    "If the P-value is even smaller – less than 1% – the result is called \"highly statistically significant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this convention, our p-value of 2.75% is considered small and the result is indeed 'statistically significant'. Therefore, the null hypothesis in our experiment is not supported and the difference between the number of heads cannot be concluded to be solely due to chance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
